Sistema de Monitoreo Ambiental ‚Äî Instalaci√≥n

Repositorio con pipelines para ingerir y procesar datos clim√°ticos (IMHPA, ETESA), enriquecimiento geoespacial y utilidades relacionadas.

# Sistema de Monitoreo Ambiental ‚Äî Instalaci√≥n

> Repositorio con pipelines para ingerir y procesar datos clim√°ticos (IMHPA, ETESA), enriquecimiento geoespacial y utilidades relacionadas.

## Instalaci√≥n (pip / venv)

1. Crear y activar el virtualenv:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Instalar dependencias runtime:

```bash
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

3. (Opcional) Instalar dependencias de desarrollo / notebooks:

```bash
python -m pip install -r requirements-dev.txt
```

> Nota: si usas `conda`, puedes crear un entorno desde `conda-forge` y evitar problemas con compilaci√≥n de librer√≠as geoespaciales.

## Variables de entorno
Puedes colocar variables en un fichero `.env` en la ra√≠z. Ejemplos √∫tiles:


## Ejecutar tests

Desde la ra√≠z del repo (con `.venv` activado):

```bash
PYTHONPATH=. pytest -q
```

Si tus imports fallan por `ModuleNotFoundError: pipelines`, aseg√∫rate de ejecutar con `PYTHONPATH=.`, o lanzar el runner con `python -m pipelines.pipeline_runner`.

## Ejecutar los pipelines

Men√∫ interactivo (recomendado para desarrollo):

```bash
python -m pipelines.pipeline_runner
```

Ejecutar todo program√°ticamente:

```bash
python -c "import pipelines.pipeline_runner as pr; pr.run_all_pipelines()"
```

Ejecutar un pipeline concreto (ejemplo IMHPA realtime):

```bash
python -m pipelines.imhpa.realtime_temp
```

## Estructura de datos en disco

Los m√≥dulos que escriben datos se encargan de crear sus carpetas con `os.makedirs(..., exist_ok=True)`. Adem√°s, el `pipeline_runner` crea las carpetas base al iniciar la ejecuci√≥n completa.

## Notas sobre problemas comunes

## Contribuir


Si quieres, puedo a√±adir una secci√≥n de `environment.yml` (conda) o un README en ingl√©s.

Sistema de Monitoreo Ambiental ‚Äî Instalaci√≥n

Este repositorio contiene pipelines para ingerir y procesar datos clim√°ticos (IMHPA, ETESA), enriquecimiento geoespacial y utilidades relacionadas.

## Requisitos previos
- macOS / Linux / Windows con Python 3.10+ (se prob√≥ con 3.11/3.12).
- Recomiendo usar un entorno virtual (`venv`) o `conda`/`mamba` para aislar dependencias.
- Dependencias nativas para `geopandas` / `osmnx`: GDAL, Fiona, PROJ, GEOS. En macOS puedes instalarlas con `brew` o usar conda-forge para evitar compilaciones.

Ejemplos (macOS):

```bash
# brew (si no usas conda)
brew install gdal proj

# o conda (recomendado para reproducibilidad geoespacial)
conda create -n sm_env python=3.11 -y
conda activate sm_env
conda install -c conda-forge geopandas osmnx jupyterlab -y
```

## Instalaci√≥n (pip / venv)

1. Crear y activar el virtualenv:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Instalar dependencias runtime:

```bash
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

3. (Opcional) Instalar dependencias de desarrollo / notebooks:

```bash
python -m pip install -r requirements-dev.txt
```

Nota: si usas `conda`, puedes omitir el paso `brew` y crear un `environment.yml` a partir de `requirements.txt` si lo deseas.

## Variables de entorno
Puedes colocar variables en un fichero `.env` en la ra√≠z. Ejemplos √∫tiles:

- `DATA_RAW_PATH` ‚Äî ruta ra√≠z para datos crudos (por defecto `data_raw`)
- `IMHPA_MAX_WORKERS` ‚Äî n√∫mero de hilos para procesar estaciones IMHPA (por defecto `8`)
- `TERRAIN_MAX_WORKERS` ‚Äî hilos para enriquecimiento de terreno
- `OSM_TILE_KM` y `OSM_MAX_WORKERS` ‚Äî controlan el tiling y concurrencia en descargas OSM

## Ejecutar tests

Desde la ra√≠z del repo (con `.venv` activado):

```bash
PYTHONPATH=. pytest -q
```

Si tus imports fallan por `ModuleNotFoundError: pipelines`, aseg√∫rate de ejecutar con `PYTHONPATH=.` o de lanzar el runner con `python -m pipelines.pipeline_runner`.

## Ejecutar los pipelines

Men√∫ interactivo (recomendado para desarrollo):

```bash
python -m pipelines.pipeline_runner
```

Ejecutar todo program√°ticamente:

```bash
python -c "import pipelines.pipeline_runner as pr; pr.run_all_pipelines()"
```

Algunos pipelines individuales tambi√©n se pueden ejecutar directamente (por ejemplo IMHPA realtime):

```bash
python -m pipelines.imhpa.realtime_temp
```

## Estructura de datos en disco
- `data_raw/` ‚Äî datos crudos y caches (ej.: `data_raw/imhpa`, `data_raw/etesa`, `data_raw/osm`)
- `data_clean/` ‚Äî salidas limpias y datasets intermedios (ej.: `data_clean/imhpa`, `data_clean/master`)

Los m√≥dulos que escriben datos se encargan de crear sus carpetas con `os.makedirs(..., exist_ok=True)`. Adem√°s, el `pipeline_runner` crea las carpetas base al iniciar la ejecuci√≥n completa.

## Notas sobre problemas comunes
- Geopandas/osmnx: si la instalaci√≥n falla por dependencias nativas, usa conda-forge o instala `gdal`/`proj` v√≠a `brew`.
- Overpass / OSM: el downloader tiene mecanimos de cache en `data_raw/osm` y descarga por tiles para evitar consultas demasiado grandes.
- Si tienes problemas con permisos al crear carpetas, revisa permisos del directorio de trabajo o ejecuta con un usuario con permisos adecuados.

## Contribuir
- Abrir issues para bugs o mejoras.
- Para cambios grandes, crear una rama y enviar un pull request.

---

## üß™ Ejecuci√≥n de Notebooks (Flujo de trabajo)

Esta secci√≥n describe el orden recomendado para ejecutar los notebooks del proyecto, una vez que los pipelines ya han sido ejecutados correctamente.

‚ö†Ô∏è Importante: Todos los notebooks dependen de los datasets generados en la carpeta data_clean/.
Primero deben ejecutarse los pipelines.

## üîπ Paso 1: Ejecutar el pipeline principal

Ejecuta el pipeline que contiene toda la ingesta y procesamiento de datos clim√°ticos (IMHPA / ETESA):

python -m pipelines.pipeline_runner


Selecciona la opci√≥n para ejecutar todos los pipelines

Espera a que el proceso termine completamente

Este paso genera los datasets limpios en data_clean/

## üîπ Paso 2: Limpieza y validaci√≥n de datos

Luego de que el pipeline finaliza, abre el notebook encargado de la limpieza y validaci√≥n:

üìì Notebook:

data_clean.ipynb (o equivalente)

En este notebook:

Se revisa la data generada

Se limpian valores nulos o inconsistentes

Se consolida el dataset final que usar√°n los an√°lisis posteriores

## üîπ Paso 3: An√°lisis y visualizaci√≥n IMHPA

Despu√©s, ejecuta el notebook de an√°lisis exploratorio:

üìì Notebook:

analisis_imhpa.ipynb

Aqu√≠ se realiza:

Visualizaci√≥n de datos clim√°ticos

An√°lisis por estaci√≥n

Exploraci√≥n de tendencias hist√≥ricas

## üîπ Paso 4: Series de tiempo y mapas clim√°ticos

Ejecuta el notebook de series de tiempo y mapas:

üìì Notebook:

serie_de_tiempo.ipynb

Este notebook:

Carga autom√°ticamente el dataset limpio desde data_clean/

Muestra mapas clim√°ticos para el a√±o 2025

Incluye la simulaci√≥n clim√°tica para 2026

Genera mapas interactivos con Folium

## üîπ Paso 5: Entrenamiento y visualizaci√≥n de modelos

Finalmente, ejecuta el notebook donde se entrenan y visualizan los modelos de Machine Learning:

üìì Notebook:

train_and_visualise.ipynb

En este notebook:

Se entrenan los modelos de sequ√≠as y inundaciones

Se usan algoritmos de Machine Learning

Se visualizan resultados y m√©tricas

Se generan los modelos finales utilizados por el sistema

## para resumir el flujo de ejecuciones 
Ejecutar pipelines 1

Ejecutar an√°lisis IMHPA

Ejecutar notebook de limpieza (data_clean)

Ejecutar series de tiempo y mapas (2025 / 2026)

Ejecutar entrenamiento de modelos (sequ√≠as e inundaciones)
